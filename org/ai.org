#+TITLE: Midterm Units
* Reasoning Under Uncertainty
  [[file:ai_materials/slides/midterm2/bayesian-slides.pdf][Bayesian Slides]]


** Probabilistic Reasoning

*** Probabilities

      [[file:ai_materials/slides/midterm2/bayesian-slides.pdf#page=11][Bayesian Slides Probability]]

**** Definitions

***** Basics
     - *Probility* is the proportion of times something is true, 
     or a degree of belief in something

     - Probabilities range in value from *0* to *1*.

     - A *random variable* can take on values on a given *domain*.

     - Since the value of the random variable is unknown, we assign a *probability* to each value.

       - This set of probabilities defines a *probability distribution*.

     - The *sample space* is the domain of a random variable.

     - An *atomic event* is a possible situation.

     - A *[[file:ai_materials/slides/midterm2/bayesian-slides.pdf#page=24][joint probability distribution]]* is the vector distribution of 
       several different random variables.
      
       - The distribution itself is ~P(X, Y, Ø)~

       - The atomic situation is ~P(X = 1, Y = 2, Ø = 3)~, which reads
	 "X = 1 AND Y = 2 AND Ø"

     - An *a priori* or *unconditional* does not depend on any condition

   * Conditional Probability
    
 # #+BEGIN_EXPORT html
 # <a href="https://www.codecogs.com/eqnedit.php?latex=P(A|B)&space;=&space;\frac{P(A,&space;B))}{P(B))}&space;\textrm{&space;if&space;}&space;P(B)&space;\neq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(A|B)&space;=&space;\frac{P(A,&space;B))}{P(B))}&space;\textrm{&space;if&space;}&space;P(B)&space;\neq&space;0" title="P(A|B) = \frac{P(A, B))}{P(B))} \textrm{ if } P(B) \neq 0" /></a>
 # #+END_EXPORT

**** Law of total probability
 #+BEGIN_SRC
    P(B) =  ∑P(B,Ai) over all possible Ai 
         =  ∑P(B|Ai)P(Ai) over all possible Ai
 #+END_SRC

**** [[file:ai_materials/slides/midterm2/bayesian-slides.pdf#page=35][Conditional Probability]]
 #+BEGIN_SRC
    P(A|B) =  P(A, B) / P(B) if P(B) =/= 0
 #+END_SRC
 Intuitively, =A | B= exists in the world of B: so =P(B) = 1= in this world.

 Thus, the probability of the intersection =P(A, B)= (read A *and* B) must be 
 scaled to the size of the whole reality, which is =P(B)=.

 TODO A picture would be nice.

**** [[file:ai_materials/slides/midterm2/bayesian-slides.pdf#page=35][Bayes Rule]]
 #+BEGIN_SRC
    P(A|B) * P(B) = P(B|A) * P(A) 
 #+END_SRC

 Note: There's an /alpha/ trick

**** [[file:ai_materials/slides/midterm2/bayesian-slides.pdf#page=37][Chain rule]]

 The goal is to compute *joint probabilites* using *conditional probabilities*

 #+BEGIN_SRC

 P(X1, ..., Xn) = P(Xn | X1, ..., Xn-1) * P(X1, ..., Xn-1) 

 #+END_SRC

 Test equation
 OBS abletonlive


*** Reasoning

**** Inference
> *Goal*: given evidence, compute probability of events

Inference tasks                                                  
- compute *posterior distribution* given evidence                

> *Ex*: Given the observation =( o = door )=, what is the probability
that the robot is at position =X=. =P(X | o = door) = ?=

- *choose an action* to *achieve a high reward* given some evidence
- Best *expected utility*

#+BEGIN_SRC
∑ ( P(Output = i | Action = aj, evidence) 
   x ExpectedUtility(Output = i) )
// over all possible outputs
#+END_SRC

- *classification*
Given a series of observations, aggregate the chance they are a certain class.

> *Ex*: Given client data, will they pay the mortage correctly?

Uses *Naïve Bayes*, which is =argmax P(Class=c | o)=.

- *diagnosis*                                                        

> *Ex*: probability of illness given result of analysis ( or symptoms )

**** Independence
=A= and =B= are *independent* iff =P(A, B) = P(A) * P(B)=

Which implies that =P(A | B) = P(A)=

> Independence is good. It *reduces* the size of the probability distribution.
(M * N) becomes (M + N)

Further references: /AI: A Modern Approach/ by Norvig

** Bayesian networks


** Markov Models
*** MDP
[[https://aulaglobal.uc3m.es/mod/url/view.php?id=1625904][Videos on MDP]]

